<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SAI   METIS Presentation - Vedaang Chopra</title>
    <link rel="stylesheet" href="https://vedaangchopra.live/theme/css/style.css" />
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header>
            <nav>
                <a href="https://vedaangchopra.live/" class="profile-brand">
                    <img src="https://vedaangchopra.live/images/profile.png" alt="Profile" class="profile-circle">
                </a>
                <a href="https://vedaangchopra.live/" class="brand">Home</a>
                <a href="https://vedaangchopra.live/projects.html">Projects</a>
                <a href="resume.html">Resume</a>
                <a href="blog.html">Writing</a>
                <a href="uses.html">Uses</a>
                <button id="theme-toggle" aria-label="Toggle Theme">
                    <span class="icon">â˜€</span>
                </button>
            </nav>
        </header>

        <main>
<article>
    <header>
        <h1>SAI   METIS Presentation</h1>
        <div class="meta">
            <span class="date">2025-12-12</span>
        </div>
    </header>
    
    <div class="content">
        <div class="download-box" style="margin-bottom: 2rem; padding: 1rem; background: var(--btn-bg); border-radius: 8px; display: inline-block;">
    <a href="https://vedaangchopra.live/blog/CS 8803  SAI - METIS Presentation.pptx" style="text-decoration: none; font-weight: bold;">
        ğŸ“¥ Download Original Slides (PPTX)
    </a>
</div>

<h2>METIS: Fast Quality-Aware RAG Systems with</h2>
<p>Configuration Adaptation</p>
<p>Hello all, today I will  be presenting the paper METIS, from researchers at Princeton and University of Chicago</p>
<p><strong>Key Takeaways:</strong>
*   Presented by:- Vedaang Chopra</p>
<h2>Presentation Flow</h2>
<p>Before I begin let me go through the presentation flow of how the paper is divided, so to understand the paper majorly there are 4 sections  The first section is on  A brief background on RAG; An introduction to the problem and some of its configurations The second section is about the Metis architecture The third section are some implementation details of the paper, and its results In the end we can wrap up with some conclusions and the strengths and Limitations of the paper</p>
<h2>Section 1 â€“ Introduction &amp; Background</h2>
<h2>What is Retrieval Augmented Generation(RAG) ?</h2>
<p>Large language models (LLMs) â€” like GPT or LLaMA â€” are very capable,  but they have two weaknesses: Lack of  domain-specific knowledge (e.g., financial data, medical details). Lack of access to recent or dynamic information (e.g., todayâ€™s news). [Debatable] So we use RAG that before answering the question, we first search a large database, pick the most relevant text passages, and then uses them to craft its response. Itâ€™s already used in: QA systems like ChatGPTâ€™s â€œBrowse with Bingâ€ Chatbots that need company knowledge bases Search systems that summarize documents</p>
<p><strong>Key Takeaways:</strong>
*   RAG (Retrieval-Augmented Generation) means that instead of relying only on what the model remembers from training, we retrieve extra knowledge (called â€œchunksâ€ of text) from a large database before generating an answer. Example:
*   If you ask, â€œWhen was Appleâ€™s latest iPhone released?â€ â†’ The LLM retrieves recent tech news snippets (retrieval). â†’ Then, it reads those snippets and generates a concise answer (generation).
*   RAG â€” retrieval + reasoning â†’ better answers.</p>
<h2>What problem does this paper aim to solve?</h2>
<p>â€œRAG systems make LLMs more accurate by adding external knowledge â€” but this comes at a cost: they get slower. So thereâ€™s a constant trade-off between quality and speed.â€ â€œThe first challenge is that natural language queries are vague. A question like â€˜Compare NVIDIAâ€™s operating costâ€¦â€™ doesnâ€™t specify how many documents or chunks are needed â€” the system has to figure that out.â€ â€œSecond, there are many configuration options â€” how many chunks to retrieve, how to combine them, whether to summarize. Trying every combination for every query would be computationally infeasible.â€ â€œFinally, scheduling interacts with configuration â€” even if you choose a good setup, GPU memory or batching constraints may delay the query. The best configuration also depends on available system resources.â€ â€œSo, as shown in the small table, quality improves when we use more context, but that slows things down; using fewer chunks speeds things up but risks missing key information.â€ â€œIn short, RAG designers need to balance accuracy versus latency, and this paper â€” METIS â€” proposes a systematic way to do that.â€ RAG designers must balance accuracy vs. latency.</p>
<p><strong>Key Takeaways:</strong>
*   RAG systems boost quality, but they also slow things down. Thereâ€™s a trade-off in RAG between quality and speed, which is a Hard Problem. Some of the problems include : -
*   Natural language queries are vague. Queries like â€œCompare NVIDIAâ€™s operating costâ€¦â€ doesnâ€™t tell the system how many chunks are required.
*   RAG systems have multiple configurations leading to exponential combinations. Testing all combinations for every query would be too slow.
*   Scheduling interacts with configuration:- The amount of resources available and the configuration applied for a particular query are directly related.</p>
<h2>What did past RAG systems focus on? What did they miss?</h2>
<p>Prior works as mentioned in the paper, focused on major two things, either improving the quality of answer, either fetching the right chunks or optimizing the total latency of the system. They didnâ€™t look at the overall balance of accuracy and speed of the system â€œBefore METIS, a lot of RAG research focused on only one side of the trade-off â€” either making RAG faster or improving its answer quality.â€ â€œThe speed-focused systems worked on the serving layer â€” optimizing GPU scheduling, batching similar queries, or memory usage to cut latency.â€ â€œThe quality-focused systems tried to tune RAG configurations â€” like deciding how many chunks to retrieve or how to summarize them for better answers.â€ â€œBut hereâ€™s the major gap: none of these methods tried to jointly optimize quality and delay. They either got better accuracy at the cost of speed, or faster responses with lower accuracy.â€ â€œMETIS is the first to tackle this balance directly â€” optimizing both together, dynamically per query.â€</p>
<p><strong>Key Takeaways:</strong>
*   There are already many RAG-related systems (the citations [2, 17, 32, 34, 37, 40, 44, 54, 76, 87, 90]) that mainly do two things:
*   Earlier research mostly focused on one side of the trade-off:
*   Speed-focused methods â†’ Focus on model selection or serving optimization. Examples include: -
*   Optimized Scheduling or GPU Memory usage to make queries faster.
*   Batching similar queries to save GPU time.
*   Quality-focused methods â†’ tuned RAGâ€™s internal configurations. Example include: -
*   How many chunks to retrieve ?
*   How to summarize them for better answers?
*   MAJOR LIMITATION: - Prior Works didnâ€™t focus on the  trade-off between response delay and generation quality</p>
<h2>What are the main RAG configuration â€œknobsâ€?</h2>
<p>Knobs are simply put some configurations of RAG systems. The authors are focused on 3 knobs across the paper that are quite general and are used across different RAG systems.  Number of Chunks: - It is like deciding how many pages of a document you have to share with the LLM for your question. Too few chunks â†’ not enough information â†’ wrong or incomplete answer. Too many chunks â†’ extra reading â†’ slower response, and even confusion (irrelevant text can reduce accuracy).   Synthesis_method: - Once you have the chunks, how do you pass it is as input to the model.  Stuff: - Basically you have 3 chunks pass all of them to the model. LLM reads everything at once. If input is small it is fine, for large chunks it is a problem as the model can â€œforgetâ€ details in the middle â€” the lost-in-the-middle problem. Map-Rerank :- Here pass all the chunks to the model separately, and pick the answer where model is most confident. This a fast and light method and works well if answers are contained in one chunk. Fails if info is spread across multiple chunks as model will not be able to combine facts. Map-Reduce:- Each chunk is summarized separately (Map phase). Those summaries are combined, and the LLM produces the final answer (Reduce phase).Great for complex reasoning across multiple chunks. Slower and uses more computation. Quality depends on how long the summaries are.  Summary length: - This only matters when you use map-reduce. Short summaries â†’ faster, but might lose important details. Long summaries â†’ more accurate but slower.</p>
<p><strong>Key Takeaways:</strong>
*   Every RAG query has parameters (or knobs) that control how itâ€™s run:
*   Number of Chunks
*   Synthesis Method
*   Stuff:- Concatenate all chunks and feed them to the LLM together
*   Map-Rerank: - LLM reads each chunk separately, answers for each, and then we pick the most confident answer
*   Map-Reduce: - Each chunk is summarized separately (Map phase). Those summaries are combined, and the LLM produces the final answer (Reduce phase).
*   Summary length:- This only matters when you use map-reduce</p>
<h2>How do these configurations affect quality and delay?</h2>
<p>â€œThis part of the paper shows why RAG configuration tuning is so important â€” the authors run controlled experiments to test how different settings change both response time and answer quality.â€ â€œThey used three queries from the MuSiQue dataset, which has reasoning-based QA tasks. Q1 is a simple single-fact question, Q2 needs comparing multiple entities, and Q3 requires multi-step reasoning about Voyager 1 â€” so we cover simple to complex queries.â€ â€œThen they vary three configuration knobs: 1ï¸âƒ£ The synthesis method â€” whether the LLM reads chunks separately (map_rerank), all at once (stuff), or summarizes first (map_reduce). 2ï¸âƒ£ The number of retrieved chunks â€” from 1 to 35. 3ï¸âƒ£ The summary length in map_reduce â€” from 1 to 100 words.â€ â€œThey measure F1 score for quality and response delay for speed, and plot the results in Figure 4.â€ â€œThe goal is to observe how each knob affects performance â€” and they find that the optimal configuration shifts depending on query complexity. This motivates the need for per-query configuration adaptation, which is what METIS later does.</p>
<p><strong>Key Takeaways:</strong>
*   Goal:  Test how changing each RAG configuration knob affects response time vs. quality.
*   Experiment Done: -  3 queries from MuSiQue (reasoning QA) dataset was selected
*   Q1: simple single-fact question - â€œIn what county was William W. Blair born?â€
*   Q2: moderate, cross-chunk comparison: - â€œAre Alison Skipper, Diane Gilliam Fisher, and Rachel McAdams from the same country?â€
*   Q3: complex, multi-step reasoning- â€œWhen and why did Voyager 1, the spacecraft that detected storms on Neptune, leave our solar system?â€
*   Experiments run:
*   Vary synthesis method â†’ map_rerank, stuff, map_reduce
*   Vary number of retrieved chunks â†’ from 1 to 35
*   Vary intermediate summary length (in map_reduce) â†’ 1 to 100 words
*   Measurement:
*   Track F1 score (quality) and response delay (seconds) for each query.
*   Observe how optimal configurations shift by query complexity.</p>
<p>As we can see that for q1, a   Each knob affects both quality and delay.But they also interact with each other â€” changing one affects the others. Example: If you increase num_chunks, maybe you need to switch from stuff â†’ map_reduce (because reading all chunks together becomes too long). If you shorten intermediate_length, maybe quality drops but delay improves. This is why optimizing RAG performance isnâ€™t trivial â€” itâ€™s a multi-dimensional trade-off. â€œThis figure shows the core finding of the paper â€” how each RAG configuration knob affects the trade-off between F1-score (quality) and delay (speed).â€ â€œEach plot isolates one knob: (a) changes the synthesis method, (b) changes the number of retrieved chunks, and (c) changes the summary length.â€ â€œFrom the first plot â€” different synthesis methods work best for different query types: simple Q1 performs best with map_rerank, while more complex queries like Q2 and Q3 need stuff or map_reduce to combine multiple pieces of information.â€ â€œIn the second plot â€” the optimal number of chunks depends on query complexity. Adding too many chunks increases delay and even hurts quality because of the lost-in-the-middle problem.â€ â€œIn the third plot â€” summary length matters: short summaries are fine for easy questions, but longer summaries preserve reasoning context for complex ones.â€ â€œOverall takeaway: thereâ€™s no single best configuration â€” every query has its own sweet spot between accuracy and latency, which motivates the need for a dynamic system like METIS.â€</p>
<p><strong>Key Takeaways:</strong>
*   RESULTS: -
*   Different synthesis methods suit different query types â€” simple queries prefer map_rerank, while complex reasoning needs stuff or map_reduce.
*   Optimal number of chunks varies per query â€” adding too many causes â€œlost-in-the-middle,â€ hurting both quality and speed.
*   Summary length matters â€” short summaries work for easy questions; longer ones preserve reasoning for complex ones.</p>
<h2>Why per-query tuning matters ?</h2>
<p>We need to tune per-query. But we canâ€™t just brute force our way, because even just with some simple less combination in our knobs, the combination space will explode.  Hence, we need a system that: Quickly narrows down the huge configuration space Chooses good configurations cheaply and dynamically Thatâ€™s exactly what METIS will do in the next section. â€œNow that weâ€™ve seen how each knob affects quality and delay, the next question is â€” why not just tune the best configuration per query?â€ â€œThis figure shows exactly that: when we adapt RAG settings per query, we achieve up to 3Ã— lower delay for the same quality.â€ â€œStatic configurations, shown by the Pareto boundary in blue, canâ€™t keep up â€” to reach similar latency, they lose at least 10% in F1-score.â€ â€œSo, per-query adaptation clearly helps â€” but brute-forcing all possible knob combinations is infeasible, since the configuration space grows exponentially.â€ â€œThatâ€™s where METIS comes in â€” it builds a system that can narrow down this huge space efficiently and pick the right configuration dynamically for each query.â€</p>
<p><strong>Key Takeaways:</strong>
*   Per-query tuning achieves up to 3Ã— lower delay for the same quality.
*   Static configs canâ€™t keep up â€” to match the same delay, they lose â‰¥ 10% in quality.</p>
<h2>Section 2 â€“ METIS System Design</h2>
<h2>What is METIS and what makes it different?</h2>
<p>â€œNow that weâ€™ve seen the motivation for per-query tuning, METIS is the system designed to make that practical.â€ â€œMETIS doesnâ€™t replace the LLM â€” it acts as a RAG controller, deciding how the LLM should process each query. Think of it as an autopilot that manages the RAG pipeline dynamically.â€ â€œIt has three main components: 1ï¸âƒ£ LLM Profiler â€” analyzes the incoming query and predicts its complexity, reasoning type, and information needs. 2ï¸âƒ£ Configuration Space Pruner â€” uses that profile to narrow down from thousands of possible RAG settings to just a few promising ones. 3ï¸âƒ£ Joint Scheduler â€” picks the best configuration that fits current GPU memory and load, so we balance delay and quality.â€ â€œThe diagram shows this flow: the query first goes through the profiler â†’ pruned configuration space â†’ joint scheduler â†’ and finally to retrieval and synthesis using the chosen settings.â€ â€œSo what makes METIS different is this end-to-end adaptivity â€” it tunes configurations per query while being aware of system resources, something earlier RAG systems never did.â€</p>
<p><strong>Key Takeaways:</strong>
*   METIS is a RAG controller â€” it doesnâ€™t replace the LLM; it controls how RAG runs. It has 3 major components: -
*   LLM Profiler
*   Configuration Space
*   Joint Scheduler</p>
<h2>How does METIS estimate what a query needs?</h2>
<p>The first thing METIS does for each query is to create a â€œprofileâ€ of the question. Query Complexity â€” â€œIs this question hard or simple?â€  Output: High / Low Low = simple â€œlookupâ€ questions like â€œWho is NVIDIAâ€™s CEO;  High = explanatory or multi-step questions like â€œWhy did Voyager 1 leave the solar system?â€ Joint Reasoning Requirement â€” â€œDo I need to combine multiple facts?â€ Output: Yes / No No = one chunk contains the full answer.  e.g., â€œWhat is Teslaâ€™s headquarters address?â€ Yes = you must connect data from several chunks.  e.g., â€œCompare Teslaâ€™s and Fordâ€™s 2023 profits.â€ Pieces of Information Required â€” â€œHow many separate items must I look at?â€ Numeric estimate (1â€“10). e.g., For â€œCompare NVIDIAâ€™s profits across 3 quarters,â€ â†’ 3 pieces. Length of Summarization Needed â€” â€œHow much should I condense each chunk?â€ 30 â€“ 200 words typically. Complex, data-heavy queries need longer summaries; simple ones can get by with shorter ones. Modern LLMs are good at analyzing language structure and intent. They can tell the difference between a factual lookup (â€œWhoâ€) and a reasoning question (â€œWhy,â€ â€œCompare,â€ â€œSummarizeâ€).</p>
<p><strong>Key Takeaways:</strong>
*   METIS creates profile for every query. This profile is a short summary that tells the system:
*   Query Complexity: - How complex the question is ? Output: Low / High ?
*   Joint Reasoning Requirement: - Whether those pieces(chunks) must be reasoned about together ? Output: Yes / No
*   Pieces of Information Required: - How many pieces of information it might need ? Output: - Numeric estimate (1â€“10).
*   Length of Summarization Needed: - And how much summarization is necessary. Output : - 30 â€“ 200 words typically</p>
<h2>How does METIS estimate what a query needs(contd.)?</h2>
<p>Here shows the prompt that the authors use to get the answers to the 4 questions that help profile each query.  Also another example shows the metadata about the dataset that is passed while profiling that influences the LLMâ€™s decision   They explicitly state: â€œWe use a very simple promptâ€¦ we donâ€™t perform any prompt tuning or optimizations.â€ Because their goal is not to craft the perfect prompt â€” itâ€™s to show that even a straightforward natural-language instruction can yield strong profiling accuracy when paired with the METIS mapping and scheduling pipeline.  The authors also tested feeding the profiler more data (like which embedding model is used). It didnâ€™t improve results much â€” because embedding models behave similarly. So they stick to simple, high-level metadata.</p>
<p><strong>Key Takeaways:</strong>
*   f"""
*   For the given query = {get.query()}:
*   Analyse the language and internal structure of the query and provide the following information :
*   1. Does it need joint reasoning across multiple documents or not?
*   2. Provide a complexity profile for the query:
*   Complexity: High/Low
*   Joint Reasoning needed: Yes/No
*   3. Does this query need input chunks to be summarized, and if yes, provide a range in words for the summarized chunks.
*   4. How many pieces of information are needed to answer the query?
*   database_metadata = {get.metadata()}
*   chunk_size = {get.chunk_size()}
*   Estimate the query profile along with the database_metadata and chunk_size to provide the output.
*   """
*   Example â€” for KG RAG FinSec
*   def get_metadata():
*   metadata = "The dataset consists of multiple chunks of information from Fortune 500 companies on financial reports from every quarter of 2023. The chunk size is 1024 tokens."
*   return metadata</p>
<h2>How does METIS convert profiles into configurations?</h2>
<p>Algorithm: -  If no joint reasoning â†’ use map_rerank. Else if joint reasoning and low complexity â†’ use stuff. Else (joint + high complexity) â†’ consider stuff or map_reduce. Set num_chunks range to [n, 3n]: Lower bound n ensures you at least try to retrieve one chunk per needed piece of info. Upper bound 3n gives wiggle room because retrievers often need 2â€“3Ã— redundancy to reliably grab the right info. Also leaves options for the scheduler to pick what fits GPU memory. Set intermediate_length to the profilerâ€™s suggested range.  Why does this configuration output are ranges ?  Keeps quality high (we donâ€™t prune away good options) Keeps search small (50â€“100Ã— fewer configs) Leaves room for the scheduler to choose what fits current GPU memory</p>
<p><strong>Key Takeaways:</strong>
*   METIS doesnâ€™t ask that LLM to output exact knob values. Instead, it uses a cheap, rule-based mapper that converts those 4 signals into ranges for the three knobs:
*   synthesis_method âˆˆ {map_rerank, stuff, map_reduce}
*   num_chunks âˆˆ [n, 3n]
*   intermediate_length âˆˆ profilerâ€™s suggested range (used only if map_reduce is an option)
*   Why not let the LLM output exact knob values?
*   Because that would require continuous re-training of the profiler to adapt to new pipelines and options (expensive, brittle).</p>
<h2>How does METIS choose configurations that fit available GPU memory?</h2>
<p>Question: - Now we have a small set of good configs for this query. Which single config should we run right now? Answer: Pick the best one that fits GPU memory now (to avoid queuing), while staying inside the pruned, quality-safe set. Why? Because even a â€œcomputationally lighterâ€ method can be slower overall if it doesnâ€™t fit in memory and must wait. Why joint scheduling matters (vs. separating decisions) If you pick â€œtheoretically fastestâ€ config without checking memory (baseline), you can end up waiting, which makes it slower end-to-end. METIS avoids this by coupling config choice with live resource checks. Figure 8 intuition stuff is usually faster if it fits (one big prompt). But stuff is memory-hungry: long inputs (many chunks) can exceed available VRAM â†’ the request waits in queue. map_reduce runs in smaller slices (mappers, then reducer). Even if it needs more total compute, its pieces fit into the current free memory and can start immediately. Result: lower wall-clock delay.</p>
<p><strong>Key Takeaways:</strong>
*   The selection heuristic (â€œbest-fitâ€)
*   For every candidate config in the pruned set, estimate memory need (mainly from num_chunks and, if relevant, intermediate_length).
*   Check current free GPU memory for the running batch.
*   Among configs that fit right now, pick the one with the highest memory footprint (i.e., the richest that still fits).
*   Within a quality-safe pruned set, â€œslightly more expensiveâ€ often correlates with slightly better quality (e.g., 6 chunks &gt; 5 chunks), so take the best that fits now.
*   Example: pruned says num_chunks âˆˆ [5, 10], and both 5 and 6 fit â€” pick 6.
*   Run it immediately; donâ€™t pick a config that would overflow memory (that would queue and inflate delay).
*   In short: Avoid queuing by picking the fattest config that fits now inside the pre-vetted (quality-safe) space.</p>
<h2>How does METIS handle edge cases and Relearning ?</h2>
<p>Sometimes a question is too vague for any model (or human) to profile accurately. Example: â€œCompare current U.S. stock-market trends.â€ For reliability of profiler: - Every LLM internally outputs a log-probability (log-prob) for each generated token. That number measures how confident the model is about its own answer. So METIS uses those log-prob values as a proxy for confidence in the profile.They found empirically If confidence â‰¥ threshold (â‰ˆ 90 %), then trust the profile. If confidence &lt; threshold, treat the profile as unreliable and ignore it. Use the â€œpruned configuration spaceâ€ from the last 10 successful queries. For Relearning: -  For scheduling a config when no GPU space is left: - METIS falls back gracefully using the profile signals: If no joint reasoning â†’ use map_rerank with as many chunks as fit. If joint reasoning â†’ try stuff or map_reduce with fewer chunks that fit. It can also honor SLOs (e.g., strict latency budgets) by choosing the cheapest viable option. This â€œloose decouplingâ€ keeps the profile-guided quality intent while still respecting system constraints.</p>
<p><strong>Key Takeaways:</strong>
*   There are certain edge cases that METIS also handles: -
*   How METIS detects when the profiler is unreliable.
*   How METIS recovers or learns from those cases to improve next time.
*   What if no configuration fits in the GPU ?</p>
<p>So overall this is the flow of the architecture METIS. For every query a small 7B model first profiles the query, then â€œThis diagram summarizes the end-to-end flow of how METIS works behind the scenes for every query.â€ â€œStep 1 â€“ The process begins with a query and a small metadata summary of the dataset. A lightweight LLM Profiler (like GPT-4o or LLaMA-70B) analyzes the query â€” it estimates: â€¢ how complex it is, â€¢ whether joint reasoning is needed, â€¢ how many pieces of information are required, and â€¢ how much summarization might help.â€ â€œStep 2 â€“ These four high-level outputs form the query profile. Then, using a rule-based mapping, METIS translates that profile into a reduced configuration space â€” it picks possible synthesis methods, a range for number of chunks, and a range for summary length.â€ â€œStep 3 â€“ Finally, the Joint Scheduler looks at current GPU memory and chooses the single best configuration from that reduced space â€” the one that fits in memory while maintaining high quality.â€ â€œSo, for each query, METIS goes from plain text â†’ profile â†’ narrowed configuration â†’ best-fit execution, adapting in real time to both the query and the system resources.â€</p>
<h2>Section 3 â€“  Implementation &amp; Evaluation</h2>
<h2>How is METIS implemented in practice?</h2>
<p>For implementation details of METIS It is built on top of vLLM engine, and it's just addition of some lines of code, so very light and modular The Profiler LLM is a python Class compatible with openAi library, and huggingface API, any opensource LLM can be used for the profiler, the prompt needs to be passed properly and you will get the output The Retriever from vector DB is implemented using Cohere-embed-v3.0 on FAISS db. With chunks fetched from DB, they use langchain chaining to perform the synthesis that is stuff, map_reduce, map_rerank For GPU memory information they use pytorch and pynvml for estimation</p>
<h2>What datasets and metrics are used to evaluate METIS?</h2>
<p>They build a classic RAG index: split docs into chunks (LangChain), embed with Cohere-embed-v3.0, store/search with FAISS IndexFlatL2, then send a Poisson stream of queries to simulate load.â€œTo fairly test METIS, the authors use four diverse RAG datasets â€” each representing a different query style and reasoning need.â€ â€œThey include: ğŸŸ¢ SQuAD â€” simple single-hop QA, one paragraph answers. ğŸ”µ Musique â€” multi-hop reasoning, combines facts from multiple sources. ğŸŸ£ KG RAG FinSec â€” financial document-level QA, needs multi-chunk retrieval. ğŸŸ  QMSUM â€” summarization-based QA on meeting transcripts.â€ â€œFor models, they use Mistral-7B-v3 and Llama-3.1-70B, both quantized for efficient inference. Hardware: dual-GPU NVIDIA A40 server with 384GB RAM.â€ â€œMetrics are split into two parts â€” â€¢ Quality: measured with F1-score, standard for QA tasks. â€¢ System performance: measured by delay (latency) and dollar cost, showing practical benefits beyond accuracy.â€ â€œBaselines include: â€¢ vLLM â€” fixed configuration (no adaptation). â€¢ Parrot<em> â€” better batching/scheduling but static configs. â€¢ AdaptiveRAG</em> â€” adapts based on query complexity but ignores resource cost.â€ â€œThey simulate real-world load by chunking data with LangChain, embedding using Cohere-embed-v3.0, storing in FAISS, and sending a Poisson stream of queries â€” mimicking actual RAG traffic.â€</p>
<p><strong>Key Takeaways:</strong>
*   Models &amp; hardware
*   Inference models: Mistral-7B-v3 (long context 32K) and Llama-3.1-70B (long context 128K), both AWQ-quantized.
*   Box: dual-GPU NVIDIA A40 server; 384 GB RAM; dual Xeon Gold 6130; 1 GPU serves Mistral-7B-v3; 2 GPUs serve Llama-70B.
*   Metrics
*   Quality: F1 on the generated answer (standard for RAG).
*   System: Delay (response latency) and Dollar cost (to compare against â€œjust use a bigger modelâ€ strategies).
*   Datasets (give them different query styles)
*   SQuAD: single-hop reading comprehension.
*   MuSiQue: multi-hop reasoning QA.
*   KG RAG FinSec: financial doc-level QA (needs several chunks).
*   QMSUM: query-focused meeting summarization.
*   Baselines
*   vLLM (fixed configs): strong server with a static RAG setup.
*   Parrot<em>: advanced batching/scheduling but no per-query config adaptation.
*   AdaptiveRAG</em>: uses query complexity to pick RAG configs, ignores resource cost.</p>
<h2>How does METIS perform compared to existing systems?</h2>
<p>â€œNow letâ€™s look at how METIS performs against existing RAG serving systems like vLLM, Parrot<em>, and AdaptiveRAG</em>.â€ â€œAcross all four datasets â€” KG RAG FinSec, Musique, SQuAD, and QMSUM â€” we see a consistent trend: âœ… METIS achieves 1.6Ã— to 2.5Ã— lower delay compared to the baselines, âœ… while maintaining or even improving quality by 12â€“18% in F1 score.â€ â€œFor example, in KG RAG FinSec, METIS gives 16% higher F1 and 2.4Ã— faster responses; in QMSUM, itâ€™s 2.5Ã— faster at the same quality.â€ â€œThis happens because METIS adapts its configuration per query and jointly considers GPU memory â€” so it doesnâ€™t waste time waiting for resources like fixed systems do.â€ â€œIn short â€” METIS achieves faster answers without sacrificing accuracy â€” which is exactly the balance prior RAG systems struggled to achieve.â€</p>
<p><strong>Key Takeaways:</strong>
*   METIS Achieved: - Lower delay at same quality: 1.64â€“2.54Ã— faster than AdaptiveRAG<em>; vs fixed configs (Parrot</em>/vLLM) of similar delay, METIS gets +12â€“18% F1.</p>
<h2>How does METIS perform compared to existing systems?</h2>
<p>â€œThis figure focuses on throughput â€” how many queries per second each system can handle at a fixed latency.â€ â€œAcross all four datasets, METIS achieves 1.8Ã— to 4.5Ã— higher throughput than other baselines like Parrot* or vLLM.â€ â€œThe reason is simple: METIS adapts configurations per query and in real time based on GPU memory availability. It doesnâ€™t queue requests that wonâ€™t fit â€” it picks what fits now.â€ â€œIn contrast, fixed systems waste compute cycles waiting for memory to free up or processing oversized configurations. METISâ€™s joint scheduling eliminates that waste.â€ â€œSo at the same delay budget â€” say 1.8 seconds â€” METIS can handle several more queries simultaneously without losing response quality.â€  Why: It adapts configs per query and picks what fits memory now, reducing queueing and wasted compute; fixed systems canâ€™t exploit this.</p>
<p><strong>Key Takeaways:</strong>
*   METIS Achieved: - Higher throughput: 1.8â€“4.5Ã— more QPS at a fixed latency budget.</p>
<h2>Where do METISâ€™s gains come from?</h2>
<p>â€œThis slide breaks down why METIS performs better â€” where exactly the speedups and quality gains come from.â€ â€œStarting with Figure 12 â€” when they progressively add each METIS component: 1ï¸âƒ£ Using just the LLM profiler and choosing the median config gives 1.4â€“1.68Ã— delay reduction. 2ï¸âƒ£ Adding batching (like Parrotâ€™s system) gives a small boost â€” about 1.1â€“1.2Ã— more. 3ï¸âƒ£ Finally, combining that with resource-aware scheduling â€” picking the configuration that best fits current GPU memory â€” brings the total improvement to 1.45â€“1.75Ã— faster execution.â€ â€œIn Figure 13, they analyze cost efficiency â€” using bigger inference models like GPT-4o or Llama-70B doesnâ€™t help. Those fixed systems cost 2.3â€“6.8Ã— more and still get lower F1-scores compared to METIS.â€ â€œSo METISâ€™s gains come not from using a larger model â€” but from smarter system design â€” profiling, batching, and GPU-aware scheduling together.</p>
<p><strong>Key Takeaways:</strong>
*   (Figure-12) Performance with each component(ablation):
*   Use profiler + pick median inside the pruned ranges â†’ 1.4â€“1.68Ã— faster.
*   Add batching (Parrot-style)* â†’ 1.1â€“1.2Ã— extra.
*   Add resource-aware config selection (best-fit into current GPU) â†’ another 1.45â€“1.75Ã—.
*   Figure(13) Cost angle: Simply switching to a much larger inference model with fixed configs is 2.38â€“6.8Ã— more expensive and still worse F1 than METIS. Even GPT-4o with fixed configs underperforms on F1 and costs 6.8Ã— more in their comparisons.</p>
<h2>What is the Cost of Running METIS ?</h2>
<p>â€œOne of the best parts about METIS is that itâ€™s efficient â€” even though it uses a larger LLM for profiling, the cost is minimal.â€ â€œWhy? Because the profiler LLM only sees the query text and a one-line metadata summary â€” not the entire retrieved context.â€ â†’ That means the profilerâ€™s input is around 100Ã— smaller than what the main LLM processes during answer generation. â€œIt runs once per query, before retrieval â€” so its total runtime and compute footprint are very small compared to RAG inference.â€ â€œEven with a bigger model like GPT-4-level profilers, the cost of profiling is negligible compared to the gain in accuracy and delay reduction.â€ â€œIn short â€” METIS achieves the goal of using an expensive model cheaply: it leverages LLM reasoning power only where it matters â€” for query understanding, not for generation.</p>
<p><strong>Key Takeaways:</strong>
*   METIS uses a larger LLM for profiling than for generation (e.g., 7B parameter model), but thatâ€™s still cheap because the input to the profiler is tiny (just the short query + metadata).
*   Why itâ€™s cheap:
*   Query length â‰ˆ 100Ã— shorter than the retrieved context the main LLM must read.
*   Profiler runs only once per query, not on the full document.
*   Even with a bigger model, total profiling cost â‰ª RAG inference cost.
*   So METIS achieves its goal of using an expensive LLM in a cheap way â€” it only reads the short query, not the whole knowledge base.</p>
<h2>How sensitive is METIS to model or retriever changes?</h2>
<p>â€œThis slide tests how robust METIS is â€” what happens if we change the model or retriever setup?â€ â€œIn Figure 14, they test profiler feedback: Occasionally, METIS seeds the profiler with the best output from a â€˜goldenâ€™ configuration â€” the one thatâ€™s slow but very accurate. That feedback improves the profilerâ€™s future predictions, leading to a 4â€“6% boost in F1-score over time. Importantly, METIS still enforces memory limits, so it doesnâ€™t start choosing overly expensive configurations.â€ â€œIn Figure 15, they test what happens when switching to a bigger inference model, like Llama-3.1-70B. METIS remains 2.1â€“2.4Ã— faster than AdaptiveRAG<em>, even with the larger model. Fixed-config systems like Parrot</em> and vLLM fall behind by 7â€“10% in F1.â€ â€œSo overall, METIS is quite robust â€” it keeps its speed and accuracy advantages even when the underlying model or retriever setup changes.â€</p>
<p><strong>Key Takeaways:</strong>
*   (Figure 14) Profiler feedback: Occasionally seeding the profiler with the best-answer output (from a â€œgoldenâ€ expensive config) improves F1 by 4â€“6%. The scheduler still enforces memory constraints, so this doesnâ€™t spiral into always-expensive choices.
*   (Figure 15) Bigger inference LLM (Llama-3.1-70B): METIS still 2.1â€“2.4Ã— faster than AdaptiveRAG<em> at similar F1; Parrot</em>/vLLM fixed configs lag by 7â€“10% F1.</p>
<h2>Section 4 â€“ Conclusion &amp; Discussion</h2>
<h2>What are the key takeaways from METIS? What are some positives of the paper ?</h2>
<p>The core strength of METIS lies in unifying two worldsâ€”system scheduling and model quality tuning. Itâ€™s compact, practical, and complementary to existing serving optimizations like chunked prefill or KV-cache reuse. The main takeaway: METIS turns RAG from a static pipeline into an intelligent controller that balances quality and latency dynamicallyâ€”essentially an autopilot for retrieval-augmented generation.   â€œThe core idea behind METIS is simple but powerful â€” itâ€™s a RAG autopilot. It understands each query, picks the best configuration dynamically, and keeps improving over time.â€ â€œIts first strength is being the first system to jointly optimize both RAG quality and delay per query â€” no prior work has done that systematically.â€ â€œSecond, METISâ€™s adaptive configuration + resource-aware scheduling gives it 1.6â€“2.5Ã— faster responses and 12â€“18% better F1 scores than state-of-the-art baselines like Parrot<em> and AdaptiveRAG</em>.â€ â€œItâ€™s lightweight â€” only about 2K lines of Python code, built on familiar tools like vLLM, FAISS, LangChain, and PyTorch â€” making it easy to adopt.â€ â€œItâ€™s also modular and plug-and-play â€” can work with any retrieval or serving engine without needing retraining.â€ â€œFinally, itâ€™s self-improving â€” using LLM confidence scores and periodic feedback to refine its profiling over time.â€ â€œIn short â€” METIS unifies two worlds: system scheduling and model-level reasoning. It turns RAG from a static pipeline into an intelligent, adaptive controller that automatically balances accuracy and latency.â€</p>
<p><strong>Key Takeaways:</strong>
*   KEY STRENGTHS: -
*   First system to jointly optimize RAG quality and delay per query.
*   Adaptive configuration + resource-aware scheduling â†’ 1.6 â€“ 2.5Ã— faster, +12 â€“ 18 % F1.
*   Lightweight &amp; modular â€” ~2 K LOC built on vLLM, FAISS, LangChain, PyTorch.
*   Plug-and-play with any retrieval or LLM serving engine.
*   Self-improving profiler â€” uses confidence &amp; feedback to learn over time.
*   Big idea: METIS acts as a â€œRAG autopilotâ€ that understands each query, adapts on the fly, and learns continuously.</p>
<h2>What are the current limitations and future directions?</h2>
<p>METIS currently excels in classic retrievalâ†’synthesisâ†’answer setups, but future RAG systems are becoming more â€œagentic,â€ involving multiple reasoning hops. Extending METIS to coordinate configurations across such stages is an exciting open problem.  Another limitation is that METIS still treats its mapping rules heuristically â€” a learned or reinforcement-based approach could adapt better. Finally, KV-cache reuse and automatic metadata generation could further cut latency and make METIS plug-and-play for new domains.   â€œWhile METIS performs really well in todayâ€™s RAG setups, it still has some limitations that open exciting research directions.â€ â€œFirst, itâ€™s designed for standard RAG pipelines â€” single retrieval and synthesis stages. Future RAG systems are moving toward multi-agent or chain-of-thought pipelines, where multiple reasoning steps or agents collaborate. METIS doesnâ€™t yet handle that level of coordination.â€ â€œSecond, thereâ€™s no KV-cache reuse â€” which means it doesnâ€™t yet store or blend cached model states across queries. Efficient cache blending could drastically reduce latency for repeated or related queries.â€ â€œThird, METIS relies on heuristic mapping â€” a rule-based system to map LLM profile outputs to configuration knobs. A future learned or reinforcement-based mapper could make it even smarter and more adaptive.â€ â€œFor future directions, the authors suggest three key paths: 1ï¸âƒ£ Extend to multi-agent or multi-hop RAG, 2ï¸âƒ£ Integrate KV-cache blending for reuse, 3ï¸âƒ£ Auto-generate dataset metadata using LLM summarizers to make it plug-and-play.â€</p>
<p><strong>Key Takeaways:</strong>
*   Designed for standard RAG pipelines â€” not yet extended to multi-agent or chain-of-thought RAG.
*   No KV-cache reuse â€” storing blended caches across queries still an open challenge.
*   Heuristic mapping only â€” profiler + rule-mapping not fully learned or fine-tuned.
*   Future directions:
*   Extend to multi-stage / agentic RAG reasoning.
*   Integrate KV-cache blending for faster reuse.
*   Auto-generate dataset metadata using LLM summarizers.</p>
<h2>Thank you</h2>
<p>Concerns: - Across all reviews, the common questions relate to three themes â€” rule interpretability, profiler cost, and system practicality. METIS addresses these by (1) using an LLM profiler that prunes 50â€“100Ã— config space with negligible cost; (2) a joint scheduler that adapts to GPU state in real time; and (3) a fallback and feedback loop ensuring reliability. Our follow-up experiments add metadata/no-metadata ablations, profiler size sweeps, fairness and SLO tests, and a learned policy variant. Even under these stricter conditions, METIS continues to deliver 1.6â€“2.8Ã— lower latency, 1.8â€“4.5Ã— higher throughput, and â‰¤ 10 % overhead, proving its practical and general impact on RAG serving systems    Questions: -</p>
<p><strong>Key Takeaways:</strong>
*   Q and A ?</p>
    </div>
</article>
        </main>

        <footer>
            <div class="footer-content">
                <div class="footer-icons">
                    <a href="https://github.com/Vedaang-Chopra" target="_blank" class="footer-icon-btn" aria-label="GitHub">
                        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg>
                    </a>
                    <a href="https://linkedin.com/in/vedaang-chopra/" target="_blank" class="footer-icon-btn" aria-label="LinkedIn">
                        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg>
                    </a>
                    <a href="mailto:vedaangchopra1009@gmail.com" class="footer-icon-btn" aria-label="Email">
                        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg>
                    </a>
                </div>
                <div class="footer-links">
                    <p>&copy; 2026 Vedaang Chopra</p>
                </div>
            </div>
        </footer>
    </div>

    <script>
        const toggleBtn = document.getElementById('theme-toggle');
        const body = document.body;
        const icon = toggleBtn.querySelector('.icon');

        // Theme Definitions
        const themes = ['', 'mint-theme', 'dark-theme', 'sepia-theme'];
        const icons = ['â˜€', 'ğŸŒ¿', 'ğŸŒ™', 'â˜•'];

        // Load saved theme
        let currentThemeIndex = 0;
        const savedTheme = localStorage.getItem('theme');
        
        if (savedTheme) {
            const savedIndex = themes.indexOf(savedTheme);
            if (savedIndex !== -1) {
                currentThemeIndex = savedIndex;
                if (themes[currentThemeIndex]) {
                    body.classList.add(themes[currentThemeIndex]);
                }
                icon.textContent = icons[currentThemeIndex];
            }
        }

        toggleBtn.addEventListener('click', () => {
            // Remove current theme class if it exists
            if (themes[currentThemeIndex]) {
                body.classList.remove(themes[currentThemeIndex]);
            }

            // Cycle to next theme
            currentThemeIndex = (currentThemeIndex + 1) % themes.length;

            // Add new theme class if it exists (not empty string)
            if (themes[currentThemeIndex]) {
                body.classList.add(themes[currentThemeIndex]);
                localStorage.setItem('theme', themes[currentThemeIndex]);
            } else {
                localStorage.removeItem('theme');
            }

            // Update icon
            icon.textContent = icons[currentThemeIndex];
        });
    </script>
</body>
</html>