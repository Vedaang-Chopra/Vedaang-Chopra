<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Vedaang Chopra - molmo</title><link href="https://vedaangchopra.live/" rel="alternate"/><link href="https://vedaangchopra.live/feeds/molmo.atom.xml" rel="self"/><id>https://vedaangchopra.live/</id><updated>2026-02-09T10:03:00-05:00</updated><entry><title>Part 4 - When Architecture Meets Data - What MOLMO Teaches Us</title><link href="https://vedaangchopra.live/blog/molmo-part-4.html" rel="alternate"/><published>2026-02-09T10:03:00-05:00</published><updated>2026-02-09T10:03:00-05:00</updated><author><name>Vedaang Chopra</name></author><id>tag:vedaangchopra.live,2026-02-09:/blog/molmo-part-4.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Part 4 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
&lt;a href="/blog/molmo-part-3"&gt;← Previous&lt;/a&gt; | Next →&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;When Architecture Meets Data: What MOLMO Teaches Us&lt;/h1&gt;
&lt;p&gt;Up to this point, MOLMO has preserved visual information, compressed it without destroying structure, and aligned it with language in a spatially meaningful way. The final question is whether this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Part 4 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
&lt;a href="/blog/molmo-part-3"&gt;← Previous&lt;/a&gt; | Next →&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;When Architecture Meets Data: What MOLMO Teaches Us&lt;/h1&gt;
&lt;p&gt;Up to this point, MOLMO has preserved visual information, compressed it without destroying structure, and aligned it with language in a spatially meaningful way. The final question is whether this information is actually &lt;em&gt;used&lt;/em&gt; during reasoning.&lt;/p&gt;
&lt;p&gt;A VLM can have excellent visual representations and still behave like a captioning model if its decoder treats vision as a static prefix rather than an active source of evidence. MOLMO is explicit about avoiding this failure mode.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Decoder-Only, But Not Vision-Blind&lt;/h2&gt;
&lt;p&gt;MOLMO uses a &lt;strong&gt;decoder-only language model&lt;/strong&gt;. This choice is deliberate. During generation:
* &lt;strong&gt;Visual tokens are fully visible at every decoding step&lt;/strong&gt;
* &lt;strong&gt;Text tokens are causally masked&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This creates an asymmetric attention pattern:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;flowchart&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LR&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;VT&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Visual&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Tokens&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;nPersistent&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Context&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;TT1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;Token&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;TT2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;Token&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;VT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;TT2&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;TT1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;TT2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The implication is subtle but critical: &lt;strong&gt;the model can always re-attend to the entire visual scene while generating each new word.&lt;/strong&gt; Vision is not something the model "reads once and forgets." It functions as a &lt;strong&gt;persistent external memory&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Vision as Persistent Memory&lt;/h2&gt;
&lt;p&gt;Many VLM failures arise because vision is treated as a compressed hint—a short prefix or limited attention window. In such setups, early decoding decisions lock in interpretations that cannot be revised. MOLMO's decoding strategy enables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Re-checking spatial relationships mid-generation&lt;/li&gt;
&lt;li&gt;Revisiting visual evidence when resolving ambiguity&lt;/li&gt;
&lt;li&gt;Maintaining consistency between earlier and later claims&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is especially important for tasks like counting ("are there three cups or two?") or referring expressions ("the object on the left versus the center"), where the answer depends on constant verification against visual evidence.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Why Data Still Matters — But Only If Architecture Lets It&lt;/h2&gt;
&lt;p&gt;Up to this point, the discussion has focused almost entirely on architecture. This is intentional. MOLMO makes a strong implicit claim:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data does not create capabilities by itself. It only reveals the capabilities that the architecture already permits.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;PixMo, MOLMO's data suite, is best understood through this lens—as &lt;strong&gt;capability probes&lt;/strong&gt; designed to exercise specific architectural affordances, rather than merely a collection of large datasets.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;PixMo as Capability Supervision, Not Scale&lt;/h2&gt;
&lt;p&gt;Rather than enumerating PixMo datasets individually, it is more useful to group them by &lt;em&gt;what they are trying to teach the model to do&lt;/em&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Capability&lt;/th&gt;
&lt;th style="text-align: left;"&gt;PixMo Subset&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Scale&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Key Feature&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Dense captioning&lt;/td&gt;
&lt;td style="text-align: left;"&gt;PixMo-CAP&lt;/td&gt;
&lt;td style="text-align: left;"&gt;712K images, 1.3M captions&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;Voice-first&lt;/strong&gt;: annotators speak 60-90s descriptions (~196 words avg vs. 11 in COCO)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Visual Q&amp;amp;A&lt;/td&gt;
&lt;td style="text-align: left;"&gt;AskModelAnything&lt;/td&gt;
&lt;td style="text-align: left;"&gt;162K QA pairs / 73K images&lt;/td&gt;
&lt;td style="text-align: left;"&gt;Human-in-the-loop with text-only LLM (no VLM supervision)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Pointing &amp;amp; grounding&lt;/td&gt;
&lt;td style="text-align: left;"&gt;PixMo-Points&lt;/td&gt;
&lt;td style="text-align: left;"&gt;2.3M expressions / 229K images&lt;/td&gt;
&lt;td style="text-align: left;"&gt;10× larger than RefCOCO; enables "count-by-pointing"&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Caption-based reasoning&lt;/td&gt;
&lt;td style="text-align: left;"&gt;PixMo-CapQA&lt;/td&gt;
&lt;td style="text-align: left;"&gt;214K QA / 165K images&lt;/td&gt;
&lt;td style="text-align: left;"&gt;Caption → QA conversion via text LLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Document understanding&lt;/td&gt;
&lt;td style="text-align: left;"&gt;PixMo-Docs&lt;/td&gt;
&lt;td style="text-align: left;"&gt;255K images, 2.3M QA&lt;/td&gt;
&lt;td style="text-align: left;"&gt;Code-generated charts (Matplotlib, LaTeX, Mermaid, etc.)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Visual numeracy&lt;/td&gt;
&lt;td style="text-align: left;"&gt;PixMo-Clocks&lt;/td&gt;
&lt;td style="text-align: left;"&gt;826K images&lt;/td&gt;
&lt;td style="text-align: left;"&gt;Synthetic watch faces; teaches geometric → numeric reasoning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Grounded counting&lt;/td&gt;
&lt;td style="text-align: left;"&gt;PixMo-Count&lt;/td&gt;
&lt;td style="text-align: left;"&gt;36K train / 540 val&lt;/td&gt;
&lt;td style="text-align: left;"&gt;Point-based counting supervision; harder than CountBenchQA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The key insight is that &lt;strong&gt;none of these capabilities would emerge reliably&lt;/strong&gt; if MOLMO used single-resolution resizing, aggressive early pooling, or vision-as-prefix decoding. PixMo does not compensate for architectural weaknesses—it &lt;em&gt;assumes they have already been addressed&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Figure 1 from the paper shows how PixMo datasets map to MOLMO's capabilities:&lt;/p&gt;
&lt;p&gt;&lt;img alt="PixMo datasets and the capabilities they enable in MOLMO." src="/images/molmo_pixmo_datasets.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: PixMo (left) consists of three annotated datasets and four synthetic datasets, all constructed without the use of VLMs. Each dataset enables specific capabilities in MOLMO (right), from fine-grained understanding to pointing and visual skills.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;How Well Does It Work?&lt;/h2&gt;
&lt;p&gt;MOLMO's results validate the architectural philosophy:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Overall ranking&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Molmo-72B ranks &lt;strong&gt;#2&lt;/strong&gt; (behind only GPT-4o), beating Gemini 1.5 Pro and Claude 3.5 Sonnet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Counting &amp;amp; grounding&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Best-in-class due to point-then-count reasoning and 2D pointing data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;cap-F1 correlation&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Strong 0.82 correlation between captioning quality and benchmark performance—suggesting dense captioning is a reliable proxy for multimodal capability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Openness&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;First fully open VLM (weights, data, code) to reach this performance tier&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Where MOLMO is weaker: reasoning-heavy tasks (MathVista) and fine OCR, which require more structured reasoning data.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;A Common Failure Pattern in VLM Training&lt;/h2&gt;
&lt;p&gt;Many VLMs follow an implicit strategy: collect diverse multimodal data, mix everything into instruction tuning, and hope scale smooths over inconsistencies. This often leads to models that caption fluently but hallucinate spatial facts or perform poorly at grounding.&lt;/p&gt;
&lt;p&gt;MOLMO avoids this by enforcing a clean separation:
* &lt;strong&gt;Architecture&lt;/strong&gt; defines &lt;em&gt;what is possible&lt;/em&gt;.
* &lt;strong&gt;Data&lt;/strong&gt; teaches the model &lt;em&gt;when to use it&lt;/em&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;The Broader Lesson&lt;/h2&gt;
&lt;p&gt;A useful mental model is to think in terms of ceilings: a weak architecture creates a low ceiling, no matter how much data is added. PixMo pushes MOLMO toward its architectural limits, but it does not redefine those limits.&lt;/p&gt;
&lt;p&gt;The takeaway is not "data matters less," but something more precise: &lt;strong&gt;in VLMs, architecture determines &lt;em&gt;which datasets are even learnable&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;What MOLMO Teaches Us About VLM Design&lt;/h2&gt;
&lt;p&gt;With this in mind, we can step back and ask a final, forward-looking question: what does MOLMO teach us about the future design of vision-language systems?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preprocessing is modeling.&lt;/strong&gt; The image preprocessor is not infrastructure—it is the first layer of representation learning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compression must be semantically aware.&lt;/strong&gt; Token reduction should happen after, not before, semantic features are extracted.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Connectors must preserve structure.&lt;/strong&gt; Dimensionality alignment is necessary but not sufficient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Vision must remain accessible.&lt;/strong&gt; The decoder should treat visual tokens as persistent context, not a one-time input.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data reveals architecture.&lt;/strong&gt; Training data can only teach capabilities that the architecture already permits.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MOLMO demonstrates that distinct architectural choices—not just scale—will define the limits of what our models can truly understand.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Disclosure:&lt;/strong&gt; Parts of this blog series were generated with the assistance of AI tools. The content has been reviewed and curated for technical accuracy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This blog series was created with assistance from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2409.17146"&gt;MOLMO Paper&lt;/a&gt;&lt;/strong&gt; — Deitke et al., "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models" (2024)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/blog/cs8803-vlm---molmo-and-pixmo"&gt;Original Presentation Slides&lt;/a&gt;&lt;/strong&gt; — CS8803 VLM course presentation with detailed speaker notes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://openai.com/chatgpt"&gt;ChatGPT&lt;/a&gt;&lt;/strong&gt; — OpenAI's language model, used for drafting and editing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://deepmind.google/"&gt;Antigravity&lt;/a&gt;&lt;/strong&gt; — Google DeepMind's agentic coding assistant, used for structuring and publishing&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Part 4 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
&lt;a href="/blog/molmo-part-3"&gt;← Previous&lt;/a&gt; | Next →&lt;/p&gt;</content><category term="molmo"/><category term="[MOLMO"/><category term="PixMo"/><category term="VLM]"/></entry><entry><title>Part 3 - From Patches to Reasoning - Tokens, Bandwidth, and Connectors</title><link href="https://vedaangchopra.live/blog/molmo-part-3.html" rel="alternate"/><published>2026-02-09T10:02:00-05:00</published><updated>2026-02-09T10:02:00-05:00</updated><author><name>Vedaang Chopra</name></author><id>tag:vedaangchopra.live,2026-02-09:/blog/molmo-part-3.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Part 3 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
&lt;a href="/blog/molmo-part-2"&gt;← Previous&lt;/a&gt; | &lt;a href="/blog/molmo-part-4"&gt;Next →&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;From Patches to Reasoning: Tokens, Bandwidth, and Connectors&lt;/h1&gt;
&lt;p&gt;Once MOLMO has preserved visual information through multi-scale preprocessing, it faces the next architectural bottleneck: &lt;strong&gt;token explosion&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Vision Transformers do not reason over images directly—they reason over &lt;em&gt;patch tokens …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Part 3 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
&lt;a href="/blog/molmo-part-2"&gt;← Previous&lt;/a&gt; | &lt;a href="/blog/molmo-part-4"&gt;Next →&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;From Patches to Reasoning: Tokens, Bandwidth, and Connectors&lt;/h1&gt;
&lt;p&gt;Once MOLMO has preserved visual information through multi-scale preprocessing, it faces the next architectural bottleneck: &lt;strong&gt;token explosion&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Vision Transformers do not reason over images directly—they reason over &lt;em&gt;patch tokens&lt;/em&gt;. If left unchecked, MOLMO's careful preprocessing would overwhelm the language model with far more visual tokens than it can meaningfully attend to.&lt;/p&gt;
&lt;p&gt;The challenge here is subtle:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How do we aggressively compress visual information &lt;strong&gt;without destroying the very fine-grained details we preserved&lt;/strong&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;MOLMO's answer is to &lt;strong&gt;compress intelligently after semantic extraction&lt;/strong&gt;, preserving resolution through the encoding stage.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Patchification: Where Token Explosion Begins&lt;/h2&gt;
&lt;p&gt;Each 336×336 image (global view or crop) is divided into 14×14 pixel patches.
* 336 / 14 = 24 patches per side
* Total per image: &lt;strong&gt;24 × 24 = 576 patches&lt;/strong&gt;
* If MOLMO processed 9 crops + 1 global image naively, this would result in &lt;strong&gt;~5,760 visual tokens&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This is not just inefficient—it is unusable for a decoder-only LLM that must attend over all tokens during generation.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Multi-Layer Feature Extraction: Texture Meets Semantics&lt;/h2&gt;
&lt;p&gt;Before compression, MOLMO makes one important modeling decision: it extracts features from &lt;strong&gt;two internal ViT layers&lt;/strong&gt;:
* A mid-level layer → captures textures, edges, local patterns
* A late layer → captures object-level and semantic information&lt;/p&gt;
&lt;p&gt;These are combined before pooling. This reflects a researcher's intuition: fine-grained visual reasoning often depends on &lt;em&gt;both&lt;/em&gt; texture-level evidence and semantic abstraction. Discarding either prematurely harms downstream grounding.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;2×2 Attention Pooling: Semantic Compression&lt;/h2&gt;
&lt;p&gt;Instead of uniform pooling or token dropping, MOLMO applies &lt;strong&gt;2×2 attention pooling&lt;/strong&gt;:
* Every 2×2 group of neighboring patches → one pooled token
* Spatial resolution per image: 24×24 → 12×12
* 576 tokens → &lt;strong&gt;144 tokens&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This pooling uses &lt;em&gt;learned attention weights&lt;/em&gt;, enabling content-aware compression:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;flowchart&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LR&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="err"&gt;×&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Patch&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Tokens&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;n576&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;AP&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="err"&gt;×&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Attention&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Pooling&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;AP&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="err"&gt;×&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Tokens&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="n"&gt;n144&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From a modeling perspective, this is critical: tokens now represent &lt;em&gt;regions&lt;/em&gt; rather than individual pixels. Each token still corresponds to a localized part of the image, allowing the LLM to reason over regions rather than raw patches.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Removing Redundancy Across Overlapping Crops&lt;/h2&gt;
&lt;p&gt;Because crops overlap, some regions appear multiple times. MOLMO explicitly removes duplicate tokens corresponding to overlapping areas. The result is roughly &lt;strong&gt;~1100 unique visual tokens&lt;/strong&gt; for an entire high-resolution image, with no double-counting and no fragmented evidence.&lt;/p&gt;
&lt;p&gt;From an AI research perspective, this is best understood as &lt;strong&gt;visual bandwidth control&lt;/strong&gt;. The goal is to feed the LLM &lt;em&gt;the right abstractions at the right granularity&lt;/em&gt;, rather than simply more pixels.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Putting It Together: A Walkthrough&lt;/h2&gt;
&lt;p&gt;To make this concrete, consider a 1920×1080 image of a busy café street:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Step&lt;/th&gt;
&lt;th&gt;Operation&lt;/th&gt;
&lt;th&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Input image&lt;/td&gt;
&lt;td&gt;1920×1080×3 RGB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Create global view&lt;/td&gt;
&lt;td&gt;336×336 (entire scene, scaled down)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Create high-res crops&lt;/td&gt;
&lt;td&gt;~8 overlapping 336×336 tiles with 56px overlap&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Patchify each crop&lt;/td&gt;
&lt;td&gt;24×24 = 576 patches per crop (14×14px patches)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;ViT encoding&lt;/td&gt;
&lt;td&gt;Each patch → 1024-D vector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;Multi-layer extraction&lt;/td&gt;
&lt;td&gt;Mid + late layer features combined&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2×2 attention pooling&lt;/td&gt;
&lt;td&gt;576 → 144 tokens per crop&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;Remove overlap duplicates&lt;/td&gt;
&lt;td&gt;~9 crops × 144 ≈ 1,296 → &lt;strong&gt;~1,100 unique tokens&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;Connector projection&lt;/td&gt;
&lt;td&gt;1,100 × 1024-D → 1,100 × 4096-D&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;Add layout tokens&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;lt;img_start_lowres&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;row_end&amp;gt;&lt;/code&gt;, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;Concatenate with text&lt;/td&gt;
&lt;td&gt;[1,100 visual] + [~8 text tokens] → LLM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The LLM then generates text autoregressively, attending to all visual tokens at each step. When asked &lt;em&gt;"What color is the car near the café?"&lt;/em&gt;, it can localize "car" to specific visual tokens and verify "near the café" spatially—because the architecture preserved this structure throughout.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;The Connector Is Not a Projection Layer&lt;/h2&gt;
&lt;p&gt;In many VLM descriptions, the connector is dismissed in a single sentence: &lt;em&gt;"visual features are projected into the language embedding space."&lt;/em&gt; MOLMO treats this as an oversimplification—and implicitly argues that this framing is one reason many VLMs underperform at grounding and reasoning.&lt;/p&gt;
&lt;p&gt;The connector is not just about dimensionality alignment. It is about &lt;strong&gt;making visual structure legible to a language model&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;The Common Misconception&lt;/h3&gt;
&lt;p&gt;A naïve mental model of VLMs looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;flowchart&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LR&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;V&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Visual&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Tokens&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Projection&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LLM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Token&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Space&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If this were sufficient, most VLMs would reason well about space, count objects reliably, and ground references precisely. They do not. The reason is simple:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Language models do not natively understand spatial structure. If spatial information is not explicitly encoded, it is effectively invisible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2&gt;Layout Tokens: Giving Vision a Coordinate System&lt;/h2&gt;
&lt;p&gt;MOLMO distinguishes between &lt;strong&gt;alignment&lt;/strong&gt; (tokens live in the same embedding space) and &lt;strong&gt;accessibility&lt;/strong&gt; (the LLM can reliably use visual information). To bridge this gap, MOLMO augments each visual token with &lt;strong&gt;explicit layout information&lt;/strong&gt;:
* Token position within the image grid
* Which crop it originated from
* Relative spatial location&lt;/p&gt;
&lt;p&gt;This information is encoded using &lt;strong&gt;layout embeddings&lt;/strong&gt;, which are injected alongside visual features before entering the LLM.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MOLMO Spatial Tokens and Layout" src="/images/molmo_spatial.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Layout injection adds positional context to each visual token, enabling the LLM to understand spatial relationships.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The key idea is not positional encoding in the transformer sense—it is &lt;em&gt;semantic spatial grounding&lt;/em&gt;. To the LLM, these tokens are no longer anonymous vectors. They are "this region," "over here," or "adjacent to that other region."&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Why This Matters for Reasoning&lt;/h2&gt;
&lt;p&gt;From a researcher's perspective, this section is where MOLMO's architectural philosophy becomes clear: &lt;strong&gt;multimodal reasoning is not just about fusing modalities—it is about preserving the &lt;em&gt;structure&lt;/em&gt; of each modality through fusion&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Without layout-aware connectors, counting degenerates into guesswork and spatial explanations collapse into generic captions. MOLMO's connector ensures that visual tokens behave less like "extra words" and more like &lt;strong&gt;persistent, structured memory&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Part 3 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
&lt;a href="/blog/molmo-part-2"&gt;← Previous&lt;/a&gt; | &lt;a href="/blog/molmo-part-4"&gt;Next →&lt;/a&gt;&lt;/p&gt;</content><category term="molmo"/><category term="[MOLMO"/><category term="PixMo"/><category term="VLM]"/></entry><entry><title>Part 2 - Seeing Is the Bottleneck - MOLMO's Image Preprocessing</title><link href="https://vedaangchopra.live/blog/molmo-part-2.html" rel="alternate"/><published>2026-02-09T10:01:00-05:00</published><updated>2026-02-09T10:01:00-05:00</updated><author><name>Vedaang Chopra</name></author><id>tag:vedaangchopra.live,2026-02-09:/blog/molmo-part-2.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Part 2 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
&lt;a href="/blog/molmo-part-1"&gt;← Previous&lt;/a&gt; | &lt;a href="/blog/molmo-part-3"&gt;Next →&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;Seeing Is the Bottleneck: MOLMO's Image Preprocessing&lt;/h1&gt;
&lt;h2&gt;Why MOLMO's Architecture Is Quietly Radical&lt;/h2&gt;
&lt;p&gt;At first glance, MOLMO's architecture looks almost conservative. There is no novel transformer variant, no exotic fusion module, and no end-to-end multimodal pretraining trick that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Part 2 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
&lt;a href="/blog/molmo-part-1"&gt;← Previous&lt;/a&gt; | &lt;a href="/blog/molmo-part-3"&gt;Next →&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;Seeing Is the Bottleneck: MOLMO's Image Preprocessing&lt;/h1&gt;
&lt;h2&gt;Why MOLMO's Architecture Is Quietly Radical&lt;/h2&gt;
&lt;p&gt;At first glance, MOLMO's architecture looks almost conservative. There is no novel transformer variant, no exotic fusion module, and no end-to-end multimodal pretraining trick that fundamentally alters the standard VLM recipe. This is intentional.&lt;/p&gt;
&lt;p&gt;MOLMO's architectural contribution centers on &lt;strong&gt;treating known constraints as immovable facts&lt;/strong&gt; and designing around them. In particular, it takes seriously a constraint that many VLMs implicitly ignore:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Vision Transformers are square, resolution-limited models operating on patchified images—while real-world visual reasoning is neither square nor low-resolution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Most VLMs implicitly assume that resizing an image to a single fixed resolution is a harmless preprocessing step. MOLMO treats this assumption as false.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;The Core Mismatch: ViTs vs. Real Images&lt;/h2&gt;
&lt;p&gt;Vision Transformers like ViT-L/14 accept inputs of a fixed resolution (e.g., 336×336). This creates an unavoidable trade-off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resize aggressively → preserve global layout, lose fine detail.&lt;/li&gt;
&lt;li&gt;Crop aggressively → preserve detail, lose context.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most VLMs choose one side of this trade-off implicitly. MOLMO refuses to choose. Instead, it reframes the problem:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What if the vision encoder sees &lt;em&gt;multiple coherent views&lt;/em&gt; of the same image, each optimized for a different level of abstraction?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2&gt;Multi-Scale Tiling as a Representation Strategy&lt;/h2&gt;
&lt;p&gt;MOLMO's preprocessor produces &lt;strong&gt;two complementary visual representations&lt;/strong&gt; from a single image:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;A low-resolution global view&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The entire image resized to 336×336.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Preserves scene-level context, object co-occurrence, and layout.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multiple high-resolution overlapping crops&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Each crop is 336×336.&lt;/li&gt;
&lt;li&gt;Covers the image on a grid.&lt;/li&gt;
&lt;li&gt;Overlaps adjacent crops to avoid boundary artifacts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is not a data augmentation trick. It is a &lt;strong&gt;deliberate representational decomposition&lt;/strong&gt;. Figure 5 from the MOLMO paper illustrates this process:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Converting an image into tokens. The image is turned into a single low-res and several overlapping high-res crops." src="/images/molmo_tokens_sequence.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 5: Image-to-token conversion. The original image (top left) produces a low-resolution global view and multiple high-resolution crops (bottom left). Special tokens mark image start, end, and row boundaries.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Each crop answers a different question:
* &lt;em&gt;What is happening overall?&lt;/em&gt;
* &lt;em&gt;What fine details exist here?&lt;/em&gt;
* &lt;em&gt;What text or small objects would be lost otherwise?&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Why Overlap Matters (More Than It Seems)&lt;/h2&gt;
&lt;p&gt;The overlapping region between crops is not incidental. Without overlap, objects near crop boundaries get split, text gets truncated, and spatial continuity breaks. By overlapping crops, MOLMO ensures that &lt;strong&gt;any visually meaningful region appears fully in at least one crop&lt;/strong&gt;. This guarantees that the vision encoder never sees "half an object" as its best view.&lt;/p&gt;
&lt;p&gt;Figure 3 from the paper demonstrates the difference clearly:&lt;/p&gt;
&lt;p&gt;&lt;img alt="An image cropped without and with overlap. Overlapping crops ensure that central patches are encoded with neighboring context." src="/images/molmo_overlap_comparison.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: Overlap vs. no-overlap cropping. Highlighted regions show areas used by the LLM. With overlap, the bike's brand name is always fully visible in at least one crop.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;From a reasoning perspective, this is crucial. The LLM requires &lt;em&gt;complete visual evidence&lt;/em&gt; to reason effectively—fragmented patches undermine this.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Padding Is Also a Modeling Choice&lt;/h2&gt;
&lt;p&gt;Real images rarely tile perfectly. MOLMO pads edge crops when needed, but does so explicitly:
* Each patch is tagged as real image, partial padding, or full padding.
* Padding-type embeddings tell the model what is &lt;em&gt;absence&lt;/em&gt; versus &lt;em&gt;dark pixels&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This avoids a subtle but common failure mode where models confuse black padding with visual content—particularly harmful in low-light or nighttime scenes.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;The Key Insight&lt;/h2&gt;
&lt;p&gt;The key insight here is what multi-scale cropping represents: &lt;strong&gt;visual reasoning is scale-sensitive&lt;/strong&gt;. A single resolution cannot support both perception and interpretation. MOLMO treats scale as a &lt;strong&gt;first-class axis of representation&lt;/strong&gt;, rather than something the model is expected to infer implicitly.&lt;/p&gt;
&lt;p&gt;MOLMO reframes where architectural novelty should live: &lt;strong&gt;in how visual evidence is preserved, structured, and made accessible to reasoning mechanisms&lt;/strong&gt;—rather than simply in deeper encoders or larger language models.&lt;/p&gt;
&lt;p&gt;This is why MOLMO is better understood as an architectural &lt;em&gt;correction&lt;/em&gt; rather than an architectural &lt;em&gt;innovation&lt;/em&gt;. It does not add complexity; it removes implicit assumptions that were never justified.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Part 2 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
&lt;a href="/blog/molmo-part-1"&gt;← Previous&lt;/a&gt; | &lt;a href="/blog/molmo-part-3"&gt;Next →&lt;/a&gt;&lt;/p&gt;</content><category term="molmo"/><category term="[MOLMO"/><category term="PixMo"/><category term="VLM]"/></entry><entry><title>Part 1 - Where Multimodal Reasoning Actually Lives</title><link href="https://vedaangchopra.live/blog/molmo-part-1.html" rel="alternate"/><published>2026-02-09T10:00:00-05:00</published><updated>2026-02-09T10:00:00-05:00</updated><author><name>Vedaang Chopra</name></author><id>tag:vedaangchopra.live,2026-02-09:/blog/molmo-part-1.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Part 1 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
← Previous | &lt;a href="/blog/molmo-part-2"&gt;Next →&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;Where Multimodal Reasoning Actually Lives&lt;/h1&gt;
&lt;h2&gt;Why MOLMO Is Worth Studying&lt;/h2&gt;
&lt;p&gt;Most recent discussions around Vision-Language Models (VLMs) revolve around benchmarks, scale, or whether a model is "open" in name. MOLMO is interesting for a different reason. It is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Part 1 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
← Previous | &lt;a href="/blog/molmo-part-2"&gt;Next →&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;Where Multimodal Reasoning Actually Lives&lt;/h1&gt;
&lt;h2&gt;Why MOLMO Is Worth Studying&lt;/h2&gt;
&lt;p&gt;Most recent discussions around Vision-Language Models (VLMs) revolve around benchmarks, scale, or whether a model is "open" in name. MOLMO is interesting for a different reason. It is one of the few recent VLMs that can be treated as a &lt;strong&gt;complete research artifact&lt;/strong&gt;—one where data construction, architectural decisions, training choices, and evaluation all form a coherent story.&lt;/p&gt;
&lt;p&gt;This article is not a leaderboard-driven summary of MOLMO. Instead, it uses MOLMO as a &lt;strong&gt;lens to reason about VLM design itself&lt;/strong&gt;: where multimodal reasoning actually happens, why many VLMs fail silently, and how architectural decisions upstream of the language model determine what kind of intelligence is even possible downstream.&lt;/p&gt;
&lt;p&gt;The central argument is simple but often overlooked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Multimodal reasoning does not emerge inside the LLM by default. It emerges only if the architecture preserves, aligns, and exposes visual information in a form the LLM can actually reason over.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;MOLMO is valuable because it makes these constraints explicit—and largely gets them right.&lt;/p&gt;
&lt;h3&gt;Why "Open" Actually Matters Here&lt;/h3&gt;
&lt;p&gt;Many VLMs claim openness but fall short in practice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API-only&lt;/strong&gt; (GPT-4o, Gemini): No weights, no data, no reproducibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open weights, closed data&lt;/strong&gt; (Qwen-VL, InternVL): You can run it, but you cannot study &lt;em&gt;why&lt;/em&gt; it works.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open weights + distilled data&lt;/strong&gt; (LLaVA, Cambrian): Trained on synthetic captions from proprietary VLMs—meaning reproducibility still depends on closed systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MOLMO occupies a different position: &lt;strong&gt;open weights, open data (PixMo), and open training code&lt;/strong&gt;. This means the entire system—from data collection decisions to architectural choices—can be audited, reproduced, and extended. For researchers, this is not a marketing distinction; it is the difference between a benchmark result and a research artifact.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;The Core VLM Abstraction&lt;/h2&gt;
&lt;p&gt;At a high level, most VLMs look deceptively similar: an image encoder, a connector, and a language model. This simplicity hides a deeper question that determines success or failure:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where, exactly, does multimodal reasoning occur?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Not in the vision encoder alone. Not magically inside the LLM. And not in the connector by virtue of existing. Multimodal reasoning only emerges if three conditions are met:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visual information survives preprocessing&lt;/strong&gt;&lt;br&gt;
   If critical spatial or fine-grained details are destroyed before encoding, no amount of downstream reasoning can recover them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visual tokens are aligned, not merely projected&lt;/strong&gt;&lt;br&gt;
   The connector must preserve structure, locality, and layout—not just match embedding dimensions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The decoder has unrestricted access to vision during generation&lt;/strong&gt;&lt;br&gt;
   Vision must act as persistent context, not a compressed hint.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Most VLM failures trace back to violations of one or more of these constraints, often invisibly.&lt;/p&gt;
&lt;h3&gt;A Minimal VLM Abstraction&lt;/h3&gt;
&lt;p&gt;Conceptually, almost all VLMs can be reduced to the following pipeline:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;flowchart&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LR&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;VE&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Vision&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Encoder&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;VE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Vision&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Language&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Connector&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LLM&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Language&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Decoder&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Prompt&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LLM&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This diagram is misleadingly clean. The real complexity lies in what happens &lt;em&gt;inside&lt;/em&gt; each arrow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;Image → Vision Encoder&lt;/strong&gt; step determines what information is even representable.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Encoder → Connector&lt;/strong&gt; step determines what structure survives compression.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Connector → LLM&lt;/strong&gt; step determines whether vision is accessible during reasoning or merely appended as context.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MOLMO's architecture follows this standard design—connecting a vision encoder to a language model—as shown in Figure 2 from the paper:&lt;/p&gt;
&lt;p&gt;&lt;img alt="MOLMO follows the simple and standard design of connecting a vision encoder and a language model." src="/images/molmo_inference_flow.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: MOLMO's end-to-end inference flow. An image is tiled into crops, encoded, and fused with tokenized text. The LLM outputs an answer with optional pointing coordinates.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Where Multimodal Reasoning Fails&lt;/h2&gt;
&lt;p&gt;From a research perspective, the most important insight is this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The LLM cannot reason over what it cannot attend to, and it cannot attend to what the architecture has already discarded.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Consider what happens in a typical VLM pipeline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A rectangular image is resized to a fixed square resolution (e.g., 224×224 or 336×336).&lt;/li&gt;
&lt;li&gt;Fine details (text, symbols, small objects) are blurred or aliased.&lt;/li&gt;
&lt;li&gt;Spatial relationships are distorted.&lt;/li&gt;
&lt;li&gt;The vision encoder produces tokens that are already missing critical information.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point, &lt;strong&gt;the LLM has already lost&lt;/strong&gt;, regardless of how powerful it is.&lt;/p&gt;
&lt;p&gt;MOLMO's contribution lies in treating these transitions as first-class design problems rather than implementation details. In the parts that follow, we will progressively zoom in on how MOLMO addresses each of these failure modes—starting with the most underestimated part of VLMs: how images are prepared &lt;em&gt;before&lt;/em&gt; any transformer ever sees them.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Part 1 of a 4-part series on Vision-Language Model design.&lt;/strong&gt;&lt;br&gt;
← Previous | &lt;a href="/blog/molmo-part-2"&gt;Next →&lt;/a&gt;&lt;/p&gt;</content><category term="molmo"/><category term="[MOLMO"/><category term="PixMo"/><category term="VLM]"/></entry></feed>